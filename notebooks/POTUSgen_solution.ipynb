{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: POTUS Tweet generator #\n",
    "\n",
    "The goal of this task is to learn about the generation of text fragments by utilizing probabilistic language models. For this task, we provide you with a data set of scraped Twitter messages and will generate unique sentences by computing probabilities of N-grams. N-grams are sequences of $N$ consecutive words. For $N=2$ and the given text `will leave florida`, the n-grams would be `['will', 'leave'], ['leave','florida']`. This notebook includes questions for self-study which are not required to be answered but deepen the understanding of language processing.\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "- Generating n-grams from a given corpus\n",
    "- Calculate the frequency of a successing word\n",
    "\n",
    "### Passing criteria\n",
    "\n",
    "When pushing the notebook to Artemis, all tests have to succeed. Do not change any function names or arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>DISCLAIMER</h3>\n",
    "    <p>For the sake of testing and grading, we want to state that you are not allowed to import any other libraries and should not change the structure of the provided functions (i.a. the arguments and the name of the functions). Further, please make sure to use Python version 3.6 or higher.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from helpers import cleanup_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset (nothing to do here)\n",
    "\n",
    "In order to generate twitter messages, it is essential to work on real messages. For this purpose, we provide you with scraped tweets from [@realDonaldTrump](https://twitter.com/realDonaldTrump). These tweets have been additionally processed by removing any clutter (images, links etc.) and retweeted messages. \n",
    "In the first step, the saved messages are therefore imported from the `.txt` file and splitted into an array of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"Load the tweet data as string and splitting it into words\n",
    "\n",
    "    :param filename: Path to the data\n",
    "    :returns data: Tweet data as a string\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.read()\n",
    "        data = ' '.join(data.split()).lower()\n",
    "        return data\n",
    "\n",
    "data = load_data(\"../data/data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order top get a better grasp about the data set we just imported, the data is visualized in a wordcloud that shows words bigger if they tend to appear more often. The graphic below was created by this [generator](https://github.com/amueller/word_cloud).\n",
    "<img src=\"assets/masked_wordcloud.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate N-Grams\n",
    "N-grams are sequences of $ n $ words, which need to be extracted from the text first. In this task you have to split up the given dataset into a list of lists. Each sublist should have length of `len(sublist) = N` which is provided through the function call.\n",
    "\n",
    "Example (N=1):\n",
    "```python\n",
    "data = 'donald trump'\n",
    "ngrams = generate_ngram_successors(data, 1)\n",
    "\n",
    "print(ngrams)\n",
    "```\n",
    "\n",
    "This should return:\n",
    "```bash\n",
    "[['donald'], ['trump']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_successors(text, N):\n",
    "    \"\"\"Splitting the .txt file into a list of sublists of size N (N-Grams)\n",
    "\n",
    "    :param text: Parsed twitter messages as String\n",
    "    :param N: ngram parameter\n",
    "    :returns ngram_successors: Dict of sublists of size N\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize ngram_successors as a list\n",
    "    ngrams = []\n",
    "    \n",
    "    # TODO: ['i', 'will', 'be', 'leaving', 'florida']\n",
    "    # Splitting up the input file into a list of strings\n",
    "    words = text.split(\" \")\n",
    "    \n",
    "    # TODO: [['i', 'will', 'be'], ['will', 'be', 'leaving'], ['be', 'leaving', 'florida']] with N=3\n",
    "    # Splitting all words into sublists of length N\n",
    "    # and appending these to ngram_successors\n",
    "    for i in range(len(words) - N + 1):\n",
    "        ngrams.append(words[i : i + N])\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "################################### UNCOMMENT FOR TESTING ###################################\n",
    "# data = 'donald trump'\n",
    "# ngrams = generate_ngram_successors(data, 1)\n",
    "# print(ngrams)\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate the frequencies\n",
    "\n",
    "This function should return a hash map that maps a given n-gram to the respective n-gram successor and its count.\n",
    "\n",
    "Example (N=3):\n",
    "```python\n",
    "ngrams = [['will', 'leave', 'florida'], ['will', 'leave', 'nyc'], ['will', 'leave', 'florida']]\n",
    "freqs = calculate_ngram_freqs(ngrams)\n",
    "\n",
    "print(freqs)\n",
    "```\n",
    "\n",
    "This should return:\n",
    "```bash\n",
    "{'will leave': {'florida': 2, 'nyc': 1}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_freqs(ngrams):\n",
    "    \"\"\"Calculate the frequency of a subsequent word given a sequence of ngrams\n",
    "\n",
    "    :param ngrams: List of ngrams\n",
    "    :returns freqs: Dict of frequency of successors\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    freqs = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        # !TODOÂ¡\n",
    "        # Differentiate successor (= lastWord) ...\n",
    "        successor = ngram[-1]\n",
    "\n",
    "        # TODO\n",
    "        # ... from the ngram sequence\n",
    "        ngram_seq  = \" \".join(ngram[:-1])\n",
    "\n",
    "        # If a given sequence is not known, initialize \n",
    "        # an empty hash map for ngram_seq.\n",
    "        if ngram_seq not in freqs:\n",
    "            freqs[ngram_seq] = {}\n",
    "\n",
    "        # If successor not seen, set th count for \n",
    "        # the successor and the given sequence to 0.\n",
    "        if successor not in freqs[ngram_seq]:\n",
    "            freqs[ngram_seq][successor] = 0\n",
    "        \n",
    "        # TODO \n",
    "        # Increase frequency of successor given the sequence\n",
    "        freqs[ngram_seq][successor] += 1\n",
    "\n",
    "    return freqs\n",
    "\n",
    "\n",
    "######################################## UNCOMMENT FOR TESTING ########################################\n",
    "# test_gram = [['will', 'leave', 'florida'], ['will', 'leave', 'nyc'], ['will', 'leave', 'florida']]\n",
    "# print(calculate_ngram_freqs(test_gram))\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Most probable successor\n",
    "\n",
    "By finishing the previous function, we are actually able to caluclate the frequency of a successor for a given start sequence of `N-1` words. In case of Bigrams (N=3), this would mean that we would need two words to generate text. We want to test this now with the following function. Please complete the subsequent `next_word_max()` function and return the successor with the highest frequency among the possible successors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_max(prev_seq, counts, N):\n",
    "    \"\"\"Returns the word with the highest occurence given a word sequence\n",
    "\n",
    "    :param prev_seq: Previous sentence sequence\n",
    "    :param counts: Respective frequency of successing words\n",
    "    :param N: ngram parameter\n",
    "    :returns max_successor: Next successor of given prev_seq\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    token_seq = \" \".join(prev_seq.split()[-(N-1):])\n",
    "\n",
    "    # TODO\n",
    "    max_successor = max(counts[token_seq].items(), key=lambda k: k[1])[0]\n",
    "\n",
    "    return max_successor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Question</h3>\n",
    "    <p>Do you think that it is a good idea to use the implemented function to generate randomized texts? Why?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sanity Check (nothing to do here)\n",
    "\n",
    "You do not have to write any code here. With this step you only can ensure that your previous implementations work in the intended way. If you have done everything correct, this function should return you a message from the president.\n",
    "By changing the String of `start_seq`, you affect the outcome of the function and can test for yourself the most frequent successor of a given sequence. Beware to increase the parameter `N` accordingly if you test a longer start sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am very proud of you! \n"
     ]
    }
   ],
   "source": [
    "def sanity_check(data, start_seq=\"i am\"):\n",
    "    \"\"\"Checks the previous function implementations\n",
    "    \n",
    "    :param data: Loaded twitter messages\n",
    "    :param start_seq: Start sequence (Default value = \"i am\")\n",
    "    :returns generated_text: Cleaned up message as a String\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    N = 3\n",
    "    sentences = 0\n",
    "    \n",
    "    ngrams = generate_ngram_successors(data, N)\n",
    "    counts = calculate_ngram_freqs(ngrams)\n",
    "    \n",
    "    generated_text = start_seq.lower()\n",
    "    \n",
    "    while sentences < 1:\n",
    "        generated_text += \" \" + next_word_max(generated_text, counts, N)\n",
    "        sentences += 1 if generated_text.endswith(('.','!', '?')) else 0\n",
    "    \n",
    "    generated_text = cleanup_result(generated_text)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "print(sanity_check(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Weighted successors\n",
    "\n",
    "Spoiler Alert! You might have guessed it already: It might not be the best idea to only append the successor with the highest frequency to a given word or sequence of words. If we would do this everytime, we would always get back the same sentence for a given start sequence. To circumvent this issue and to be able to generate different messages with every function call, the objective of the next function is to choose out of all possible successors and taking the respective frequency into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_weighted(prev_seq, N, counts, seed=None):\n",
    "    \"\"\"Outputs the next word by taking the a weighted choice of the most recent tokens\n",
    "\n",
    "    :param prev_seq: Previous sentence sequence\n",
    "    :param N: ngram parameter\n",
    "    :param counts: Respective frequency of successing words\n",
    "    :returns next_word: Next successor of given prev_seq\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    token_seq = \" \".join(prev_seq.split()[-(N-1):])\n",
    "    choices = counts[token_seq].items()\n",
    "\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    \n",
    "    successors, frequencies = zip(*list(choices))\n",
    "\n",
    "    # TODO\n",
    "    next_word = random.choices(population=successors, weights=[f / total for f in frequencies], k=1)\n",
    "\n",
    "    return next_word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate text messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentence(data, N=3):\n",
    "    \"\"\"Generate a random sentence based on text input file\n",
    "\n",
    "    :param data: Loaded twitter messages\n",
    "    :param N: ngram parameter (Default value = 3)\n",
    "    :returns generated_text: Cleaned up message as a String\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate successors and calculate frequencies\n",
    "    ngrams = generate_ngram_successors(data, N)\n",
    "    counts = calculate_ngram_freqs(ngrams)\n",
    "\n",
    "    # Randomize start sequence and amount of sentences\n",
    "    sentence_count = random.randint(2, 5)\n",
    "    start_seq = random.choice(list(counts))\n",
    "    generated_text = start_seq.lower()\n",
    "    sentences = 0\n",
    "\n",
    "    while sentences < sentence_count:\n",
    "        generated_text += \" \" + next_word_weighted(generated_text, N, counts)\n",
    "        sentences += 1 if generated_text.endswith(('.','!', '?')) else 0\n",
    "\n",
    "    return cleanup_result(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past monday at 7:30 pm. Hes saddled our children and people like robert mueller, and around the world. \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(gen_sentence(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Questions</h3>\n",
    "    <p>1. How could this model further be advanced to produce even better results?</p>\n",
    "    <p>2. Play around with the window size N. What is the outcome? How does it affect the output?</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
